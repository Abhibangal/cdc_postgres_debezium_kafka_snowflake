# ğŸ§© Containerized CDC Ingestion Pipeline (Postgres â†’ Debezium â†’ Kafka â†’ Snowflake)

This repository contains a **containerized Change Data Capture (CDC) pipeline** built using **Docker**, **Debezium**, **Kafka**, and **Snowflake**.  
It captures **log-based changes from PostgreSQLâ€™s Write-Ahead Logs (WAL)**, streams them through Kafka, and ingests them into Snowflake â€” where a **dynamic SCD Type 2 table** is maintained.

---

## ğŸ“ Project Overview

### Components:
- **Postgres** â€“ Source database for CDC (configured with WAL-based replication)
- **Debezium** â€“ Reads WAL logs to capture insert, update, and delete events
- **Kafka & Kafka Connect** â€“ Streams CDC events and manages connectors
- **Snowflake Connector** â€“ Ingests data from Kafka topics into Snowflake tables
- **Docker Compose** â€“ Manages and orchestrates all containerized services

---

## âš™ï¸ Folder Structure

```
â”œâ”€â”€ connectors/
â”‚   â””â”€â”€ snowflake-kafka-connector.jar      # Downloaded Snowflake Kafka connector
â”‚
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ connect/
â”‚       â”œâ”€â”€ jar/                           # Contains Snowflake connector JAR
â”‚       â””â”€â”€ ...                            # Debezium connectors run here
â”‚
â”œâ”€â”€ postgres-connector.json                # Debezium connector config for Postgres (WAL-based)
â”œâ”€â”€ snowflake-connector.json               # Kafka connector config for Snowflake
â”œâ”€â”€ docker-compose.yaml                    # Docker setup for all services
â”œâ”€â”€ RUN_GUIDE.txt                          # Instructions to run the pipeline
â”œâ”€â”€ debugging_command.txt                  # Commands used for debugging
â””â”€â”€ README.md                              # You are here
```

---

## ğŸš€ Setup Instructions

### 1. Clone the Repository
```bash
git clone <your-repo-url>
cd <your-repo-name>
```

### 2. Start the Docker Containers
Spin up all containers using:
```bash
docker compose up -d
```

This starts Postgres, Kafka, Zookeeper, Debezium Connect, and other required services.

---

## ğŸ”— Connector Setup

### 3. Verify Connector Registration
After startup, check the **`register-connector`** service logs to confirm both connectors are registered:
- **Postgres connector** â†’ Captures CDC events from WAL logs  
- **Snowflake connector** â†’ Pushes the captured events into Snowflake

You can verify connector status via:
```bash
docker logs connect
```

---

## â„ï¸ Snowflake Configuration

In Snowflake:
- Created a **dynamic table** to implement **Slowly Changing Dimension (SCD) Type 2** logic.  
- Data flows from the **`raw_use_log`** table (populated from Postgres WAL events) into the **SCD2-managed table**.  
- Transformation and merge logic are defined using Snowflake SQL dynamic table queries included in this repository.

---

## ğŸ§  Debugging

If you face issues during setup or runtime, refer to:
```
debugging_command.txt
```

This file contains commonly used commands for inspecting Kafka topics, connector logs, and Docker container health.

---

## ğŸ§¾ Reference Files

- **`RUN_GUIDE.txt`** â€“ Step-by-step guide to run and validate the pipeline  
- **`debugging_command.txt`** â€“ Useful CLI commands for troubleshooting  
- **`docker-compose.yaml`** â€“ Orchestration of Postgres, Kafka, Debezium, and Snowflake  
- **`postgres-connector.json`** â€“ Debezium configuration for WAL-based CDC  
- **`snowflake-connector.json`** â€“ Kafka connector configuration for Snowflake ingestion  

---

## âœ… Data Flow Summary

```
Postgres (WAL Logs)
     â”‚
     â–¼
 Debezium (Log-based CDC Capture)
     â”‚
     â–¼
 Kafka Topics
     â”‚
     â–¼
 Snowflake (Ingestion via Connector)
     â”‚
     â–¼
 Dynamic SCD2 Table
```

---

## ğŸ§° Tools & Technologies
- **Docker / Docker Compose**
- **PostgreSQL (WAL-based CDC)**
- **Kafka / Zookeeper**
- **Debezium**
- **Snowflake**
- **Snowflake Kafka Connector**

---

## ğŸ“œ Author & Notes

This setup demonstrates an **end-to-end, log-based CDC pipeline** using PostgreSQLâ€™s **Write-Ahead Logs (WAL)** for data capture.  
It provides a reliable and scalable approach to synchronize source data changes into Snowflake with **SCD Type 2** logic for historical tracking.  

Refer to the included guide files for **running instructions**, **connector setup**, and **debugging commands**.
